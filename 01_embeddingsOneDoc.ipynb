{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI vs Local Embeddings\n",
    "\n",
    "Performance comparison\n",
    "    - OpenAI's Embeddings Model\n",
    "    - InstructorEmbedding at [Huggingface](https://huggingface.co/hkunlp/instructor-xl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in ./embeddings/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (0.0.319)\n",
      "Requirement already satisfied: openai in ./embeddings/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (0.28.1)\n",
      "Requirement already satisfied: tiktoken in ./embeddings/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (0.5.1)\n",
      "Requirement already satisfied: chromadb in ./embeddings/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (0.4.14)\n",
      "Requirement already satisfied: pypdf in ./embeddings/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (3.16.4)\n",
      "Requirement already satisfied: sentence_transformers in ./embeddings/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (2.2.2)\n",
      "Requirement already satisfied: InstructorEmbedding in ./embeddings/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (1.0.1)\n",
      "Requirement already satisfied: faiss-cpu in ./embeddings/lib/python3.10/site-packages (from -r requirements.txt (line 8)) (1.7.4)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./embeddings/lib/python3.10/site-packages (from langchain->-r requirements.txt (line 1)) (2.0.22)\n",
      "Requirement already satisfied: langsmith<0.1.0,>=0.0.43 in ./embeddings/lib/python3.10/site-packages (from langchain->-r requirements.txt (line 1)) (0.0.47)\n",
      "Requirement already satisfied: anyio<4.0 in ./embeddings/lib/python3.10/site-packages (from langchain->-r requirements.txt (line 1)) (3.7.1)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in ./embeddings/lib/python3.10/site-packages (from langchain->-r requirements.txt (line 1)) (3.8.6)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in ./embeddings/lib/python3.10/site-packages (from langchain->-r requirements.txt (line 1)) (0.6.1)\n",
      "Requirement already satisfied: pydantic<3,>=1 in ./embeddings/lib/python3.10/site-packages (from langchain->-r requirements.txt (line 1)) (2.4.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./embeddings/lib/python3.10/site-packages (from langchain->-r requirements.txt (line 1)) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./embeddings/lib/python3.10/site-packages (from langchain->-r requirements.txt (line 1)) (6.0.1)\n",
      "Requirement already satisfied: numpy<2,>=1 in ./embeddings/lib/python3.10/site-packages (from langchain->-r requirements.txt (line 1)) (1.26.1)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in ./embeddings/lib/python3.10/site-packages (from langchain->-r requirements.txt (line 1)) (4.0.3)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in ./embeddings/lib/python3.10/site-packages (from langchain->-r requirements.txt (line 1)) (8.2.3)\n",
      "Requirement already satisfied: requests<3,>=2 in ./embeddings/lib/python3.10/site-packages (from langchain->-r requirements.txt (line 1)) (2.31.0)\n",
      "Requirement already satisfied: tqdm in ./embeddings/lib/python3.10/site-packages (from openai->-r requirements.txt (line 2)) (4.66.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./embeddings/lib/python3.10/site-packages (from tiktoken->-r requirements.txt (line 3)) (2023.10.3)\n",
      "Requirement already satisfied: uvicorn[standard]>=0.18.3 in ./embeddings/lib/python3.10/site-packages (from chromadb->-r requirements.txt (line 4)) (0.23.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in ./embeddings/lib/python3.10/site-packages (from chromadb->-r requirements.txt (line 4)) (1.59.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in ./embeddings/lib/python3.10/site-packages (from chromadb->-r requirements.txt (line 4)) (4.8.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in ./embeddings/lib/python3.10/site-packages (from chromadb->-r requirements.txt (line 4)) (4.0.1)\n",
      "Requirement already satisfied: pypika>=0.48.9 in ./embeddings/lib/python3.10/site-packages (from chromadb->-r requirements.txt (line 4)) (0.48.9)\n",
      "Requirement already satisfied: posthog>=2.4.0 in ./embeddings/lib/python3.10/site-packages (from chromadb->-r requirements.txt (line 4)) (3.0.2)\n",
      "Requirement already satisfied: typer>=0.9.0 in ./embeddings/lib/python3.10/site-packages (from chromadb->-r requirements.txt (line 4)) (0.9.0)\n",
      "Requirement already satisfied: pulsar-client>=3.1.0 in ./embeddings/lib/python3.10/site-packages (from chromadb->-r requirements.txt (line 4)) (3.3.0)\n",
      "Requirement already satisfied: overrides>=7.3.1 in ./embeddings/lib/python3.10/site-packages (from chromadb->-r requirements.txt (line 4)) (7.4.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in ./embeddings/lib/python3.10/site-packages (from chromadb->-r requirements.txt (line 4)) (1.16.1)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.3 in ./embeddings/lib/python3.10/site-packages (from chromadb->-r requirements.txt (line 4)) (0.7.3)\n",
      "Requirement already satisfied: importlib-resources in ./embeddings/lib/python3.10/site-packages (from chromadb->-r requirements.txt (line 4)) (6.1.0)\n",
      "Requirement already satisfied: fastapi>=0.95.2 in ./embeddings/lib/python3.10/site-packages (from chromadb->-r requirements.txt (line 4)) (0.104.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in ./embeddings/lib/python3.10/site-packages (from chromadb->-r requirements.txt (line 4)) (0.14.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in ./embeddings/lib/python3.10/site-packages (from sentence_transformers->-r requirements.txt (line 6)) (4.34.1)\n",
      "Requirement already satisfied: torch>=1.6.0 in ./embeddings/lib/python3.10/site-packages (from sentence_transformers->-r requirements.txt (line 6)) (2.1.0)\n",
      "Requirement already satisfied: torchvision in ./embeddings/lib/python3.10/site-packages (from sentence_transformers->-r requirements.txt (line 6)) (0.16.0)\n",
      "Requirement already satisfied: scikit-learn in ./embeddings/lib/python3.10/site-packages (from sentence_transformers->-r requirements.txt (line 6)) (1.3.1)\n",
      "Requirement already satisfied: scipy in ./embeddings/lib/python3.10/site-packages (from sentence_transformers->-r requirements.txt (line 6)) (1.11.3)\n",
      "Requirement already satisfied: nltk in ./embeddings/lib/python3.10/site-packages (from sentence_transformers->-r requirements.txt (line 6)) (3.8.1)\n",
      "Requirement already satisfied: sentencepiece in ./embeddings/lib/python3.10/site-packages (from sentence_transformers->-r requirements.txt (line 6)) (0.1.99)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in ./embeddings/lib/python3.10/site-packages (from sentence_transformers->-r requirements.txt (line 6)) (0.17.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./embeddings/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 1)) (1.9.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./embeddings/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 1)) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./embeddings/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 1)) (1.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in ./embeddings/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 1)) (3.3.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./embeddings/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 1)) (6.0.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./embeddings/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 1)) (23.1.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./embeddings/lib/python3.10/site-packages (from anyio<4.0->langchain->-r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: idna>=2.8 in ./embeddings/lib/python3.10/site-packages (from anyio<4.0->langchain->-r requirements.txt (line 1)) (3.4)\n",
      "Requirement already satisfied: exceptiongroup in ./embeddings/lib/python3.10/site-packages (from anyio<4.0->langchain->-r requirements.txt (line 1)) (1.1.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./embeddings/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain->-r requirements.txt (line 1)) (3.20.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in ./embeddings/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain->-r requirements.txt (line 1)) (0.9.0)\n",
      "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in ./embeddings/lib/python3.10/site-packages (from fastapi>=0.95.2->chromadb->-r requirements.txt (line 4)) (0.27.0)\n",
      "Requirement already satisfied: filelock in ./embeddings/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers->-r requirements.txt (line 6)) (3.12.4)\n",
      "Requirement already satisfied: fsspec in ./embeddings/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers->-r requirements.txt (line 6)) (2023.9.2)\n",
      "Requirement already satisfied: packaging>=20.9 in ./embeddings/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers->-r requirements.txt (line 6)) (23.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./embeddings/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain->-r requirements.txt (line 1)) (2.4)\n",
      "Requirement already satisfied: protobuf in ./embeddings/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb->-r requirements.txt (line 4)) (4.24.4)\n",
      "Requirement already satisfied: flatbuffers in ./embeddings/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb->-r requirements.txt (line 4)) (23.5.26)\n",
      "Requirement already satisfied: coloredlogs in ./embeddings/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb->-r requirements.txt (line 4)) (15.0.1)\n",
      "Requirement already satisfied: sympy in ./embeddings/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb->-r requirements.txt (line 4)) (1.12)\n",
      "Requirement already satisfied: six>=1.5 in ./embeddings/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb->-r requirements.txt (line 4)) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>2.1 in ./embeddings/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb->-r requirements.txt (line 4)) (2.8.2)\n",
      "Requirement already satisfied: backoff>=1.10.0 in ./embeddings/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb->-r requirements.txt (line 4)) (2.2.1)\n",
      "Requirement already satisfied: monotonic>=1.5 in ./embeddings/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb->-r requirements.txt (line 4)) (1.6)\n",
      "Requirement already satisfied: certifi in ./embeddings/lib/python3.10/site-packages (from pulsar-client>=3.1.0->chromadb->-r requirements.txt (line 4)) (2023.7.22)\n",
      "Requirement already satisfied: pydantic-core==2.10.1 in ./embeddings/lib/python3.10/site-packages (from pydantic<3,>=1->langchain->-r requirements.txt (line 1)) (2.10.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in ./embeddings/lib/python3.10/site-packages (from pydantic<3,>=1->langchain->-r requirements.txt (line 1)) (0.6.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./embeddings/lib/python3.10/site-packages (from requests<3,>=2->langchain->-r requirements.txt (line 1)) (2.0.7)\n",
      "Requirement already satisfied: jinja2 in ./embeddings/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers->-r requirements.txt (line 6)) (3.1.2)\n",
      "Requirement already satisfied: networkx in ./embeddings/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers->-r requirements.txt (line 6)) (3.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in ./embeddings/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers->-r requirements.txt (line 6)) (0.4.0)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in ./embeddings/lib/python3.10/site-packages (from typer>=0.9.0->chromadb->-r requirements.txt (line 4)) (8.1.7)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./embeddings/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain->-r requirements.txt (line 1)) (1.0.0)\n",
      "Requirement already satisfied: h11>=0.8 in ./embeddings/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb->-r requirements.txt (line 4)) (0.14.0)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in ./embeddings/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb->-r requirements.txt (line 4)) (1.0.0)\n",
      "Requirement already satisfied: httptools>=0.5.0 in ./embeddings/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb->-r requirements.txt (line 4)) (0.6.1)\n",
      "Requirement already satisfied: websockets>=10.4 in ./embeddings/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb->-r requirements.txt (line 4)) (11.0.3)\n",
      "Requirement already satisfied: watchfiles>=0.13 in ./embeddings/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb->-r requirements.txt (line 4)) (0.21.0)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in ./embeddings/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb->-r requirements.txt (line 4)) (0.18.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in ./embeddings/lib/python3.10/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb->-r requirements.txt (line 4)) (10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./embeddings/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->sentence_transformers->-r requirements.txt (line 6)) (2.1.3)\n",
      "Requirement already satisfied: joblib in ./embeddings/lib/python3.10/site-packages (from nltk->sentence_transformers->-r requirements.txt (line 6)) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./embeddings/lib/python3.10/site-packages (from scikit-learn->sentence_transformers->-r requirements.txt (line 6)) (3.2.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./embeddings/lib/python3.10/site-packages (from sympy->onnxruntime>=1.14.1->chromadb->-r requirements.txt (line 4)) (1.3.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./embeddings/lib/python3.10/site-packages (from torchvision->sentence_transformers->-r requirements.txt (line 6)) (10.1.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the '/Users/ccugutrillague/Documents/perso/doctorado/experiments/embeddings-for-retrievalQA/embeddings/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the environment variables and import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "openai.api_key  = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import PyPDFLoader, TextLoader, AirbyteJSONLoader\n",
    "from langchain.document_loaders import DirectoryLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ccugutrillague/Documents/outreach/smp-hackthaton/smp-llm/.venv/lib/python3.11/site-packages/InstructorEmbedding/instructor.py:7: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import trange\n"
     ]
    }
   ],
   "source": [
    "# InstructorEmbedding \n",
    "from InstructorEmbedding import INSTRUCTOR\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "# OpenAI Embedding\n",
    "from langchain.embeddings import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Multiple files from Directory (json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import AirbyteJSONLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"/Users/ccugutrillague/Documents/outreach/smp-hackthaton/smp-llm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load all repos\n",
    "from langchain.document_loaders.generic import GenericLoader\n",
    "from langchain.document_loaders.parsers import LanguageParser\n",
    "from langchain.text_splitter import Language\n",
    "loader = GenericLoader.from_filesystem(\n",
    "    root_dir + \"/mpdl_collection\",\n",
    "    glob=\"**/*\",\n",
    "    suffixes=[\".json\"],\n",
    "    parser=LanguageParser(language=Language.PYTHON, parser_threshold=500),\n",
    ")\n",
    "documents = loader.load()\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide and split text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON, chunk_size=2000, chunk_overlap=200\n",
    ")\n",
    "texts = python_splitter.split_documents(documents)\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RetrievalQA\n",
    "\n",
    "We need to store the documents in a way we can semantically search for their content.\n",
    "\n",
    "The most common approach is to embed the contents of each document then store the embedding and document in a vector store. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "db = Chroma.from_documents(texts, OpenAIEmbeddings(disallowed_special=()))\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"mmr\",  # Also test \"similarity\"\n",
    "    search_kwargs={\"k\": 8},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.chains import ConversationalRetrievalChain\n",
    "# from langchain.chat_models import ChatOpenAI\n",
    "# from langchain.memory import ConversationSummaryMemory\n",
    "\n",
    "# llm = ChatOpenAI(model_name=\"gpt-4\")\n",
    "# memory = ConversationSummaryMemory(\n",
    "#     llm=llm, memory_key=\"chat_history\", return_messages=True\n",
    "# )\n",
    "# qa = ConversationalRetrievalChain.from_llm(llm, retriever=retriever, memory=memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = \"How can I initialize a ReAct agent?\"\n",
    "# result = qa(question)\n",
    "# result[\"answer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Embeddings for MPDL document(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import faiss # for similarilty: https://faiss.ai/index.html\n",
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_embeddings(docs, embeddings, sotre_name, path):\n",
    "    vectorStore = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "    with open(f\"{path}/faiss_{sotre_name}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(vectorStore, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(sotre_name, path):\n",
    "    with open(f\"{path}/faiss_{sotre_name}.pkl\", \"rb\") as f:\n",
    "        VectorStore = pickle.load(f)\n",
    "    return VectorStore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HF Instructor Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "instructor_embeddings = HuggingFaceInstructEmbeddings(model_name=\"hkunlp/instructor-xl\",\n",
    "                                                      model_kwargs={\"device\": \"cpu\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ccugutrillague/Documents/perso/doctorado/experiments/embeddings-for-retrievalQA/Embedding_store\n"
     ]
    }
   ],
   "source": [
    "Embedding_store_path = f\"{root_dir}/Embedding_store\"\n",
    "print(Embedding_store_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<langchain.vectorstores.faiss.FAISS object at 0x16b0db010>\n"
     ]
    }
   ],
   "source": [
    "db_instructEmbedd = FAISS.from_documents(texts, instructor_embeddings)\n",
    "print(db_instructEmbedd) ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db_instructEmbedd.as_retriever(search_kwargs={\"k\":3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tags=['FAISS', 'HuggingFaceInstructEmbeddings'] vectorstore=<langchain.vectorstores.faiss.FAISS object at 0x16b0db010> search_kwargs={'k': 3}\n"
     ]
    }
   ],
   "source": [
    "print(retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'similarity'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.search_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='}\\n    ],\\n    \"download_url\": [\\n        {\\n            \"result\": {\\n                \"value\": \"https://github.com/MPDL/FirstAuthor/releases\",\\n                \"type\": \"Url\"\\n            },\\n            \"confidence\": 1,\\n            \"technique\": \"GitHub_API\"\\n        }\\n    ],\\n    \"programming_languages\": [\\n        {\\n            \"result\": {\\n                \"value\": \"Java\",\\n                \"name\": \"Java\",\\n                \"type\": \"Programming_language\",\\n                \"size\": 22899\\n            },\\n            \"confidence\": 1,\\n            \"technique\": \"GitHub_API\"\\n        }\\n    ],\\n    \"readme_url\": [\\n        {\\n            \"result\": {\\n                \"value\": \"https://raw.githubusercontent.com/MPDL/FirstAuthor/main/README.md\",\\n                \"type\": \"Url\"\\n            },\\n            \"confidence\": 1,\\n            \"technique\": \"file_exploration\"\\n        }\\n    ],\\n    \"full_title\": [\\n        {\\n            \"result\": {\\n                \"type\": \"String\",\\n                \"value\": \"AppAllAuthors\"', metadata={'source': 'Documents/MPDL_FirstAuthor_2023-11-28.json'})"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = retriever.get_relevant_documents(\"Who are the authors of this software?\")\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the chain to answer questions\n",
    "qa_chain_instrucEmbed = RetrievalQA.from_chain_type(llm=OpenAI(temperature=0.2, ),\n",
    "                                                    chain_type=\"stuff\",\n",
    "                                                    retriever=retriever,\n",
    "                                                    return_source_documents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI's embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_openAIEmbedd= FAISS.from_documents(texts, embeddings)\n",
    "retriever_openai = db_openAIEmbedd.as_retriever(search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the chain to answer questions\n",
    "qa_chain_openai = RetrievalQA.from_chain_type(llm=OpenAI(temperature=0.2, ),\n",
    "                                                    chain_type=\"stuff\",\n",
    "                                                    retriever=retriever_openai,\n",
    "                                                    return_source_documents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing both MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cite sources\n",
    "\n",
    "import textwrap # text wrapping and filling\n",
    "def wrap_text_preserve_newlines(text, width=79):\n",
    "    # Split the input text into lines based on newline characters\n",
    "    lines = text.split(\"\\n\")\n",
    "\n",
    "    #wrap each line individually\n",
    "    wrapped_lines = [textwrap.fill(line, width) for line in lines]\n",
    "\n",
    "    # Join the wrapped lines back into a single string using newline characters\n",
    "    wrapped_text = '\\n'.join(wrapped_lines)\n",
    "    return wrapped_text\n",
    "\n",
    "def process_llm_response(llm_response):\n",
    "    print(wrap_text_preserve_newlines(llm_response['result']))\n",
    "    print('\\nSources:')\n",
    "    for source in llm_response['source_documents']:\n",
    "        print(source.metadata['source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------Instructor Embeddings-------------\n",
      "\n",
      " The creator of the github repo is MPDL.\n",
      "\n",
      "Sources:\n",
      "Documents/MPDL_FirstAuthor_2023-11-28.json\n",
      "Documents/MPDL_FirstAuthor_2023-11-28.json\n",
      "Documents/MPDL_FirstAuthor_2023-11-28.json\n"
     ]
    }
   ],
   "source": [
    "query = 'Who is the creator of the github repo?'\n",
    "\n",
    "print('--------------Instructor Embeddings-------------\\n')\n",
    "llm_response = qa_chain_instrucEmbed(query)\n",
    "process_llm_response(llm_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------OpenAI Embeddings------------------\n",
      " The creator of the github repo is MPDL.\n",
      "\n",
      "Sources:\n",
      "Documents/MPDL_FirstAuthor_2023-11-28.json\n",
      "Documents/MPDL_FirstAuthor_2023-11-28.json\n",
      "Documents/MPDL_FirstAuthor_2023-11-28.json\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = 'Who is the creator of the github repo?'\n",
    "\n",
    "print('-------------------OpenAI Embeddings------------------')\n",
    "llm_response = qa_chain_openai(query)\n",
    "process_llm_response(llm_response)\n",
    "print('\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------Instructor Embeddings------------------\n",
      "\n",
      " The creator of the github repo is MPDL.\n",
      "\n",
      "Sources:\n",
      "Documents/MPDL_FirstAuthor_2023-11-28.json\n",
      "Documents/MPDL_FirstAuthor_2023-11-28.json\n",
      "Documents/MPDL_FirstAuthor_2023-11-28.json\n"
     ]
    }
   ],
   "source": [
    "query = 'Who is the creator of the github repo?'\n",
    "\n",
    "print('-------------------Instructor Embeddings------------------\\n')\n",
    "llm_response = qa_chain_instrucEmbed(query)\n",
    "process_llm_response(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------Instructor Embeddings------------------\n",
      "\n",
      " The creator of the github repo is MPDL.\n",
      "\n",
      "Sources:\n",
      "Documents/MPDL_FirstAuthor_2023-11-28.json\n",
      "Documents/MPDL_FirstAuthor_2023-11-28.json\n",
      "Documents/MPDL_FirstAuthor_2023-11-28.json\n"
     ]
    }
   ],
   "source": [
    "query = \"Who is the creator of the github repo?\"\n",
    "\n",
    "# print('-------------------OpenAI Embeddings------------------')\n",
    "# llm_response = qa_chain_openai(query)\n",
    "# process_llm_response(llm_response)\n",
    "# print('\\n\\n\\n')\n",
    "print('-------------------Instructor Embeddings------------------\\n')\n",
    "llm_response = qa_chain_instrucEmbed(query)\n",
    "process_llm_response(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------Instructor Embeddings------------------\n",
      "\n",
      " The file format of the document is a string.\n",
      "\n",
      "Sources:\n",
      "Documents/MPDL_FirstAuthor_2023-11-28.json\n",
      "Documents/MPDL_FirstAuthor_2023-11-28.json\n",
      "Documents/MPDL_FirstAuthor_2023-11-28.json\n"
     ]
    }
   ],
   "source": [
    "query = \"what is the file format of the document?\"\n",
    "\n",
    "# print('-------------------OpenAI Embeddings------------------')\n",
    "# llm_response = qa_chain_openai(query)\n",
    "# process_llm_response(llm_response)\n",
    "# print('\\n\\n\\n')\n",
    "print('-------------------Instructor Embeddings------------------\\n')\n",
    "llm_response = qa_chain_instrucEmbed(query)\n",
    "process_llm_response(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------OpenAI Embeddings------------------\n",
      " The file format of the document is Markdown (MD).\n",
      "\n",
      "Sources:\n",
      "Documents/MPDL_FirstAuthor_2023-11-28.json\n",
      "Documents/MPDL_FirstAuthor_2023-11-28.json\n",
      "Documents/MPDL_FirstAuthor_2023-11-28.json\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"what is the file format of the document?\"\n",
    "\n",
    "print('-------------------OpenAI Embeddings------------------')\n",
    "llm_response = qa_chain_openai(query)\n",
    "process_llm_response(llm_response)\n",
    "print('\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------Instructor Embeddings------------------\n",
      "\n",
      " I don't know.\n",
      "\n",
      "Sources:\n",
      "Documents/MPDL_FirstAuthor_2023-11-28.json\n",
      "Documents/MPDL_FirstAuthor_2023-11-28.json\n",
      "Documents/MPDL_FirstAuthor_2023-11-28.json\n"
     ]
    }
   ],
   "source": [
    "query = \"what license is using?\"\n",
    "\n",
    "# print('-------------------OpenAI Embeddings------------------')\n",
    "# llm_response = qa_chain_openai(query)\n",
    "# process_llm_response(llm_response)\n",
    "# print('\\n\\n\\n')\n",
    "print('-------------------Instructor Embeddings------------------\\n')\n",
    "llm_response = qa_chain_instrucEmbed(query)\n",
    "process_llm_response(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------OpenAI Embeddings------------------\n",
      " I don't know.\n",
      "\n",
      "Sources:\n",
      "Documents/MPDL_FirstAuthor_2023-11-28.json\n",
      "Documents/MPDL_FirstAuthor_2023-11-28.json\n",
      "Documents/MPDL_FirstAuthor_2023-11-28.json\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"what license is using?\"\n",
    "\n",
    "print('-------------------OpenAI Embeddings------------------')\n",
    "llm_response = qa_chain_openai(query)\n",
    "process_llm_response(llm_response)\n",
    "print('\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chains import ConversationalRetrievalChain, LLMChain\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-cpp-python\n",
      "  Downloading llama_cpp_python-0.2.20.tar.gz (8.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in ./.venv/lib/python3.11/site-packages (from llama-cpp-python) (4.8.0)\n",
      "Requirement already satisfied: numpy>=1.20.0 in ./.venv/lib/python3.11/site-packages (from llama-cpp-python) (1.26.2)\n",
      "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
      "  Obtaining dependency information for diskcache>=5.6.1 from https://files.pythonhosted.org/packages/3f/27/4570e78fc0bf5ea0ca45eb1de3818a23787af9b390c0b0a0033a1b8236f9/diskcache-5.6.3-py3-none-any.whl.metadata\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n",
      "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.20-cp311-cp311-macosx_14_0_arm64.whl size=1670022 sha256=bb8b8b40804abea0a21be4a4f479f76b4c38867e18e22aff4d933cf7d0838c66\n",
      "  Stored in directory: /Users/ccugutrillague/Library/Caches/pip/wheels/15/27/bb/3a7f3b6b9ebaf4c784eadeb5655351cca5b6d1746976f24b67\n",
      "Successfully built llama-cpp-python\n",
      "Installing collected packages: diskcache, llama-cpp-python\n",
      "Successfully installed diskcache-5.6.3 llama-cpp-python-0.2.20\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-cpp-python\n",
      "  Downloading llama_cpp_python-0.2.20.tar.gz (8.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting typing-extensions>=4.5.0 (from llama-cpp-python)\n",
      "  Obtaining dependency information for typing-extensions>=4.5.0 from https://files.pythonhosted.org/packages/24/21/7d397a4b7934ff4028987914ac1044d3b7d52712f30e2ac7a2ae5bc86dd0/typing_extensions-4.8.0-py3-none-any.whl.metadata\n",
      "  Downloading typing_extensions-4.8.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting numpy>=1.20.0 (from llama-cpp-python)\n",
      "  Obtaining dependency information for numpy>=1.20.0 from https://files.pythonhosted.org/packages/2e/54/218ce51bb571a70975f223671b2a86aa951e83abfd2a416a3d540f35115c/numpy-1.26.2-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading numpy-1.26.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (115 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.1/115.1 kB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting diskcache>=5.6.1 (from llama-cpp-python)\n",
      "  Obtaining dependency information for diskcache>=5.6.1 from https://files.pythonhosted.org/packages/3f/27/4570e78fc0bf5ea0ca45eb1de3818a23787af9b390c0b0a0033a1b8236f9/diskcache-5.6.3-py3-none-any.whl.metadata\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m81.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.2-cp311-cp311-macosx_11_0_arm64.whl (14.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.8.0-py3-none-any.whl (31 kB)\n",
      "Building wheels for collected packages: llama-cpp-python\n",
      "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.20-cp311-cp311-macosx_14_0_arm64.whl size=1670020 sha256=cd2d24f4e3585dd609572939953d919ae7e7d8f705021167a1ca4feec663da2b\n",
      "  Stored in directory: /private/var/folders/cp/m1hkjgr94zv492qffqvldc9dkth62_/T/pip-ephem-wheel-cache-ansdet1d/wheels/15/27/bb/3a7f3b6b9ebaf4c784eadeb5655351cca5b6d1746976f24b67\n",
      "Successfully built llama-cpp-python\n",
      "Installing collected packages: typing-extensions, numpy, diskcache, llama-cpp-python\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.8.0\n",
      "    Uninstalling typing_extensions-4.8.0:\n",
      "      Successfully uninstalled typing_extensions-4.8.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.2\n",
      "    Uninstalling numpy-1.26.2:\n",
      "      Successfully uninstalled numpy-1.26.2\n",
      "  Attempting uninstall: diskcache\n",
      "    Found existing installation: diskcache 5.6.3\n",
      "    Uninstalling diskcache-5.6.3:\n",
      "      Successfully uninstalled diskcache-5.6.3\n",
      "  Attempting uninstall: llama-cpp-python\n",
      "    Found existing installation: llama_cpp_python 0.2.20\n",
      "    Uninstalling llama_cpp_python-0.2.20:\n",
      "      Successfully uninstalled llama_cpp_python-0.2.20\n",
      "Successfully installed diskcache-5.6.3 llama-cpp-python-0.2.20 numpy-1.26.2 typing-extensions-4.8.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install llama-cpp-python  --upgrade --force-reinstall --no-cache-dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Model path does not exist: ./models/7B/llama-model.gguf",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/ccugutrillague/Documents/outreach/smp-hackthaton/smp-llm/01_embeddingsOneDoc.ipynb Cell 49\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ccugutrillague/Documents/outreach/smp-hackthaton/smp-llm/01_embeddingsOneDoc.ipynb#X63sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mllama_cpp\u001b[39;00m \u001b[39mimport\u001b[39;00m Llama\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ccugutrillague/Documents/outreach/smp-hackthaton/smp-llm/01_embeddingsOneDoc.ipynb#X63sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m callback_manager \u001b[39m=\u001b[39m CallbackManager([StreamingStdOutCallbackHandler()])\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/ccugutrillague/Documents/outreach/smp-hackthaton/smp-llm/01_embeddingsOneDoc.ipynb#X63sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m llm \u001b[39m=\u001b[39m Llama(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ccugutrillague/Documents/outreach/smp-hackthaton/smp-llm/01_embeddingsOneDoc.ipynb#X63sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     model_path\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m./models/7B/llama-model.gguf\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ccugutrillague/Documents/outreach/smp-hackthaton/smp-llm/01_embeddingsOneDoc.ipynb#X63sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     n_ctx\u001b[39m=\u001b[39;49m\u001b[39m5000\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ccugutrillague/Documents/outreach/smp-hackthaton/smp-llm/01_embeddingsOneDoc.ipynb#X63sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     n_gpu_layers\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ccugutrillague/Documents/outreach/smp-hackthaton/smp-llm/01_embeddingsOneDoc.ipynb#X63sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     n_batch\u001b[39m=\u001b[39;49m\u001b[39m512\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ccugutrillague/Documents/outreach/smp-hackthaton/smp-llm/01_embeddingsOneDoc.ipynb#X63sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     f16_kv\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,  \u001b[39m# MUST set to True, otherwise you will run into problem after a couple of calls\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ccugutrillague/Documents/outreach/smp-hackthaton/smp-llm/01_embeddingsOneDoc.ipynb#X63sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     callback_manager\u001b[39m=\u001b[39;49mcallback_manager,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ccugutrillague/Documents/outreach/smp-hackthaton/smp-llm/01_embeddingsOneDoc.ipynb#X63sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ccugutrillague/Documents/outreach/smp-hackthaton/smp-llm/01_embeddingsOneDoc.ipynb#X63sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/outreach/smp-hackthaton/smp-llm/.venv/lib/python3.11/site-packages/llama_cpp/llama.py:915\u001b[0m, in \u001b[0;36mLlama.__init__\u001b[0;34m(self, model_path, n_gpu_layers, main_gpu, tensor_split, vocab_only, use_mmap, use_mlock, seed, n_ctx, n_batch, n_threads, n_threads_batch, rope_scaling_type, rope_freq_base, rope_freq_scale, yarn_ext_factor, yarn_attn_factor, yarn_beta_fast, yarn_beta_slow, yarn_orig_ctx, mul_mat_q, f16_kv, logits_all, embedding, last_n_tokens_size, lora_base, lora_scale, lora_path, numa, chat_format, chat_handler, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlora_path \u001b[39m=\u001b[39m lora_path\n\u001b[1;32m    914\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(model_path):\n\u001b[0;32m--> 915\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel path does not exist: \u001b[39m\u001b[39m{\u001b[39;00mmodel_path\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    917\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_model \u001b[39m=\u001b[39m _LlamaModel(\n\u001b[1;32m    918\u001b[0m     path_model\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_path, params\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_params, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose\n\u001b[1;32m    919\u001b[0m )\n\u001b[1;32m    921\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ctx \u001b[39m=\u001b[39m _LlamaContext(\n\u001b[1;32m    922\u001b[0m     model\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_model,\n\u001b[1;32m    923\u001b[0m     params\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontext_params,\n\u001b[1;32m    924\u001b[0m     verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose,\n\u001b[1;32m    925\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Model path does not exist: ./models/7B/llama-model.gguf"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "llm = Llama(\n",
    "    model_path=\"/Users/rlm/Desktop/Code/llama/code-llama/codellama-13b-instruct.Q4_K_M.gguf\",\n",
    "    n_ctx=5000,\n",
    "    n_gpu_layers=1,\n",
    "    n_batch=512,\n",
    "    f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "embeddings",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
